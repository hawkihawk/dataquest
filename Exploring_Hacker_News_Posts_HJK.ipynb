{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring Hacker News Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacker News is a site started by the startup incubator Y Combinator, where user-submitted stories (known as \"posts\") receive votes and comments, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of the Hacker News listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "[You can find the data set here](https://www.kaggle.com/datasets/hacker-news/hacker-news-posts), but note that we have reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that didn't receive any comments and then randomly sampling from the remaining submissions. \n",
    "\n",
    "**Below are descriptions of the columns:**\n",
    "- id: the unique identifier from Hacker News for the post\n",
    "- title: the title of the post\n",
    "- url: the URL that the posts links to, if the post has a URL\n",
    "- num_points: the number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "- num_comments: the number of comments on the post\n",
    "- author: the username of the person who submitted the post\n",
    "- created_at: the date and time of the post's submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Read the hacker_news.csv file in as a list of lists, assign the result to the variable hn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "opened_file = open('hacker_news.csv')\n",
    "from csv import reader\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "header = hn[0:1]\n",
    "hn = hn[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Display the first five rows of hn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']] \n",
      "\n",
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "\n",
      "\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "\n",
      "\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "\n",
      "\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "\n",
      "\n",
      "['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(header,'\\n')\n",
    "\n",
    "for x in hn[:5]:\n",
    "    print(x)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Headers from a List of Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In order to analyze our data, we need to first remove the row containing the column headers. Let's extract the first row of data, and assign it to the variable headers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = hn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Remove the first row from hn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn = hn[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n"
     ]
    }
   ],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01'], ['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12'], ['10482257', 'Title II kills investment? Comcast and other ISPs are now spending more', 'http://arstechnica.com/business/2015/10/comcast-and-other-isps-boost-network-investment-despite-net-neutrality/', '53', '22', 'Deinos', '10/31/2015 9:48']]\n"
     ]
    }
   ],
   "source": [
    "print(hn[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting \"Ask HN\" and \"Show HN\" Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now that we've removed the headers from hn, we're ready to filter our data. Since we're only concerned with post titles beginning with Ask HN or Show HN, we'll create new lists of lists containing just the data for those titles._\n",
    "\n",
    "_Create three empty lists called ask_posts, show_posts, and other_posts._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n",
      "1162\n",
      "17193\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    title = title.lower()\n",
    "    if title.startswith('ask hn'):\n",
    "        ask_posts.append(row)        \n",
    "    elif title.startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "\n",
    "#Lets check the number of posts in ask_posts, show_posts, and other_posts:\n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Average Number of Comments for Ask HN and Show HN Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Next we'll determine if Ask Posts or Show Posts receive more comments on average._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.038417431192661\n",
      "10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_ask_comment = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    num_comments = int(row[4])\n",
    "    total_ask_comment += num_comments\n",
    "\n",
    "avg_ask_comments = total_ask_comment / len(ask_posts)\n",
    "\n",
    "print(avg_ask_comments)\n",
    "\n",
    "total_show_comment = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    num_comments = int(row[4])\n",
    "    total_show_comment += num_comments\n",
    "\n",
    "avg_show_comments = total_show_comment / len(show_posts)\n",
    "\n",
    "print(avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finding the Number of Ask Posts and Comments by Hour Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_On average, ask posts receive more comments than show posts. Since ask posts are more likely to receive comments, we'll focus our remaining analysis just on these posts._\n",
    "\n",
    "_Next we'll calculate the number of ask posts created in each hour of the day, along with the number of comments received._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'09': 251,\n",
       " '13': 1253,\n",
       " '10': 793,\n",
       " '14': 1416,\n",
       " '16': 1814,\n",
       " '23': 543,\n",
       " '12': 687,\n",
       " '17': 1146,\n",
       " '15': 4477,\n",
       " '21': 1745,\n",
       " '20': 1722,\n",
       " '02': 1381,\n",
       " '18': 1439,\n",
       " '03': 421,\n",
       " '05': 464,\n",
       " '19': 1188,\n",
       " '01': 683,\n",
       " '22': 479,\n",
       " '08': 492,\n",
       " '04': 337,\n",
       " '00': 447,\n",
       " '06': 397,\n",
       " '07': 267,\n",
       " '11': 641}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "\n",
    "for row in ask_posts:\n",
    "    result_list.append(\n",
    "        [row[6], int(row[4])]\n",
    "    )\n",
    "\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    comments = row[1]\n",
    "    time = dt.datetime.strptime(date, date_format).strftime(\"%H\")\n",
    "    if time not in counts_by_hour:\n",
    "        counts_by_hour[time] = 1\n",
    "        comments_by_hour[time] = comments\n",
    "    else:\n",
    "        counts_by_hour[time] += 1\n",
    "        comments_by_hour[time] += comments\n",
    "\n",
    "comments_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Average Number of Comments for Ask HN Posts by Hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_On the previous screen, we created two dictionaries:_\n",
    "\n",
    "- **counts_by_hour:** _contains the number of ask posts created during each hour of the day._\n",
    "- **comments_by_hour:** _contains the corresponding number of comments ask posts created at each hour received._\n",
    "\n",
    "_Below we calculate the average number of comments for posts created during each hour of the day_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['09', 5.5777777777777775],\n",
       " ['13', 14.741176470588234],\n",
       " ['10', 13.440677966101696],\n",
       " ['14', 13.233644859813085],\n",
       " ['16', 16.796296296296298],\n",
       " ['23', 7.985294117647059],\n",
       " ['12', 9.41095890410959],\n",
       " ['17', 11.46],\n",
       " ['15', 38.5948275862069],\n",
       " ['21', 16.009174311926607],\n",
       " ['20', 21.525],\n",
       " ['02', 23.810344827586206],\n",
       " ['18', 13.20183486238532],\n",
       " ['03', 7.796296296296297],\n",
       " ['05', 10.08695652173913],\n",
       " ['19', 10.8],\n",
       " ['01', 11.383333333333333],\n",
       " ['22', 6.746478873239437],\n",
       " ['08', 10.25],\n",
       " ['04', 7.170212765957447],\n",
       " ['00', 8.127272727272727],\n",
       " ['06', 9.022727272727273],\n",
       " ['07', 7.852941176470588],\n",
       " ['11', 11.051724137931034]]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append(\n",
    "        [hour, comments_by_hour[hour]/counts_by_hour[hour]]\n",
    "    )\n",
    "    \n",
    "\n",
    "avg_by_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.5777777777777775, '09'],\n",
       " [14.741176470588234, '13'],\n",
       " [13.440677966101696, '10'],\n",
       " [13.233644859813085, '14'],\n",
       " [16.796296296296298, '16'],\n",
       " [7.985294117647059, '23'],\n",
       " [9.41095890410959, '12'],\n",
       " [11.46, '17'],\n",
       " [38.5948275862069, '15'],\n",
       " [16.009174311926607, '21'],\n",
       " [21.525, '20'],\n",
       " [23.810344827586206, '02'],\n",
       " [13.20183486238532, '18'],\n",
       " [7.796296296296297, '03'],\n",
       " [10.08695652173913, '05'],\n",
       " [10.8, '19'],\n",
       " [11.383333333333333, '01'],\n",
       " [6.746478873239437, '22'],\n",
       " [10.25, '08'],\n",
       " [7.170212765957447, '04'],\n",
       " [8.127272727272727, '00'],\n",
       " [9.022727272727273, '06'],\n",
       " [7.852941176470588, '07'],\n",
       " [11.051724137931034, '11']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append(\n",
    "        [row[1], row[0]]\n",
    "    )\n",
    "\n",
    "swap_avg_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Although we now have the results we need, this format makes it difficult to identify the hours with the highest values. Let's finish by sorting the list of lists and printing the five highest values in a format that's easier to read._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for Ask Posts Comments\n",
      "\n",
      "\n",
      "15:00 38.59 average comments per post\n",
      "02:00 23.81 average comments per post\n",
      "20:00 21.52 average comments per post\n",
      "16:00 16.80 average comments per post\n",
      "21:00 16.01 average comments per post\n"
     ]
    }
   ],
   "source": [
    "sorted_swap = sorted(swap_avg_by_hour, reverse=True)\n",
    "print(\"Top 5 Hours for Ask Posts Comments\")\n",
    "print(\"\\n\")\n",
    "import datetime as dt\n",
    "time_format = \"%H\" #15\n",
    "\n",
    "for row in sorted_swap[:5]:\n",
    "    average = \"{:.2f}\".format(row[0])\n",
    "    hour = row[1]\n",
    "    new_hour = dt.datetime.strptime(hour, time_format)\n",
    "    print(\"{hr}:00 {avg} average comments per post\".format(hr=hour, avg=average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my findings posts submitted 15:00 has by far a higher chance of receiving comments, with an average of 38.59 comments per post it outwins the second runner up (02:00) by over 60%.\n",
    "\n",
    "According to [the data set documentation](https://www.kaggle.com/datasets/hacker-news/hacker-news-posts), the timezone used is Eastern Time in the US. Since I am located in UTC that would mean that the peak hour for posting is 20:00 local time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
